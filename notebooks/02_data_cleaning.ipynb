{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Cleaning & Normalization\n",
        "\n",
        "This notebook cleans and normalizes the EV India 13 dataset, creating controlled vocabularies and enhancing embedding text.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Add src to path\n",
        "project_root = Path().resolve().parent\n",
        "sys.path.insert(0, str(project_root / \"src\"))\n",
        "\n",
        "from data_cleaning import clean_dataset\n",
        "\n",
        "# Set up paths\n",
        "data_path = project_root / \"data\" / \"raw\" / \"data.json\"\n",
        "output_path = project_root / \"data\" / \"processed\" / \"cleaned_data.json\"\n",
        "vocab_dir = project_root / \"data\" / \"vocabularies\"\n",
        "\n",
        "print(f\"Loading data from: {data_path}\")\n",
        "print(f\"Output will be saved to: {output_path}\")\n",
        "print(f\"Vocabularies will be saved to: {vocab_dir}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load raw data\n",
        "with open(data_path, 'r') as f:\n",
        "    raw_data = json.load(f)\n",
        "\n",
        "print(f\"Loaded {len(raw_data)} entries\")\n",
        "print(f\"\\nExample raw entry:\")\n",
        "print(json.dumps(raw_data[0], indent=2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clean dataset\n",
        "cleaned_data, vocabularies = clean_dataset(raw_data, vocab_dir)\n",
        "\n",
        "print(f\"Cleaned {len(cleaned_data)} entries\")\n",
        "print(f\"\\nCreated vocabularies:\")\n",
        "print(f\"  - {len(vocabularies['domains'])} canonical domains\")\n",
        "print(f\"  - {len(vocabularies['categories'])} categories\")\n",
        "print(f\"  - {len(vocabularies['locations'])} locations\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show example cleaned entry\n",
        "print(\"Example cleaned entry:\")\n",
        "print(json.dumps(cleaned_data[0], indent=2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show vocabularies\n",
        "print(\"=== DOMAINS ===\")\n",
        "for domain in vocabularies['domains'][:20]:  # Show first 20\n",
        "    print(f\"  - {domain}\")\n",
        "if len(vocabularies['domains']) > 20:\n",
        "    print(f\"  ... and {len(vocabularies['domains']) - 20} more\")\n",
        "\n",
        "print(f\"\\n=== CATEGORIES ===\")\n",
        "for cat in vocabularies['categories']:\n",
        "    print(f\"  - {cat}\")\n",
        "\n",
        "print(f\"\\n=== LOCATIONS ===\")\n",
        "for loc in vocabularies['locations']:\n",
        "    print(f\"  - {loc}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare original vs normalized domains for a few entries\n",
        "import pandas as pd\n",
        "df = pd.DataFrame(cleaned_data)\n",
        "\n",
        "print(\"Domain normalization examples:\")\n",
        "for idx, row in df.head(5).iterrows():\n",
        "    print(f\"\\n{row['name']}:\")\n",
        "    print(f\"  Original: {row.get('domains', [])}\")\n",
        "    print(f\"  Normalized: {row.get('domains_normalized', [])}\")\n",
        "    print(f\"  Enhanced embedding_text: {row['embedding_text'][:100]}...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save cleaned data\n",
        "output_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "with open(output_path, 'w') as f:\n",
        "    json.dump(cleaned_data, f, indent=2)\n",
        "\n",
        "print(f\"Saved cleaned data to: {output_path}\")\n",
        "print(f\"Saved vocabularies to: {vocab_dir}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
